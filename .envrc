dotenv

use flake .

export TF_VAR_name_prefix="${USER}"
export DEV_ENV_NAME_PREFIX="${TF_VAR_name_prefix}"

export DATAFORM_SCHEMA_SUFFIX=${TF_VAR_name_prefix}

# Decode service account credentials once from TF_VAR_sa_creds
# This is the single source of truth for all credential transformations
# Keep SA_CREDS_BASE64 for backward compatibility (used by Rust CLI and tests)
export SA_CREDS_BASE64="${TF_VAR_sa_creds}"

# Decode once and reuse for all downstream variables
export DBT_BIGQUERY_CREDENTIALS_JSON="$(echo "$TF_VAR_sa_creds" | base64 -d)"
export TARGET_BIGQUERY_CREDENTIALS_JSON="${DBT_BIGQUERY_CREDENTIALS_JSON}"
export DBT_BIGQUERY_PROJECT="$(echo "$DBT_BIGQUERY_CREDENTIALS_JSON" | jq -r '.project_id')"

# Write keyfile for Meltano/Airflow (non-Dagster components)
echo "$TARGET_BIGQUERY_CREDENTIALS_JSON" > meltano/keyfile.json
export GOOGLE_APPLICATION_CREDENTIALS="$(pwd)/meltano/keyfile.json"
export DBT_BIGQUERY_KEYFILE="${GOOGLE_APPLICATION_CREDENTIALS}"

# BigQuery dataset configuration
export TARGET_BIGQUERY_DATASET="${USER}_dataset"
export TARGET_BIGQUERY_LOCATION="US"
export DBT_BIGQUERY_DATASET="dbt_${USER}"

export DOCS_BUCKET_NAME="${USER}-lana-documents"
export REPORTS_BUCKET_NAME="${USER}-lana-documents"
export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://user:password@localhost:5436/pg"
export AIRFLOW__CORE__EXECUTOR=LocalExecutor
